{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42414602-35a7-4c6c-8f23-5b6b1f2e97eb",
   "metadata": {},
   "source": [
    "# Desinformaci√≥n y Deepfakes en la Era Digital"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65f3ee-e088-4b2e-8ef7-325f1ed0d991",
   "metadata": {},
   "source": [
    "Como advierte {cite:t}`toews2022deepfakes`:\n",
    "\n",
    "> Hoy estamos en un punto de inflexi√≥n. En los pr√≥ximos meses y a√±os, los deepfakes amenazan con pasar de ser una rareza de Internet a una fuerza pol√≠tica y social ampliamente destructiva. La sociedad necesita actuar ahora para prepararse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bb46d-590e-48cc-a6c4-52f86f607ddf",
   "metadata": {},
   "source": [
    "## Introducci√≥n\n",
    "\n",
    "<div align=\"justify\">\n",
    "Vivimos en una √©poca donde la tecnolog√≠a permite crear contenido falso tan convincente que es casi imposible distinguirlo de la realidad. Los deepfakes y la desinformaci√≥n representan uno de los desaf√≠os m√°s grandes de nuestro tiempo {cite}`toews2022deepfakes,lisainstitute2024deepfakes`. <strong>Desinformaci√≥n</strong> es informaci√≥n falsa o enga√±osa creada y compartida deliberadamente con la intenci√≥n de enga√±ar o manipular.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe39aba-d631-4921-a814-328960d34930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('ACVglWBYhQA', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea828273-af25-4fe1-93a0-24a730e1fdd1",
   "metadata": {},
   "source": [
    "### Tipos de informaci√≥n falsa:\n",
    "\n",
    "1. **Desinformaci√≥n (Disinformation)**\n",
    "   - Informaci√≥n falsa creada **intencionalmente** para enga√±ar.\n",
    "   - Ejemplo: Noticias falsas sobre resultados electorales.\n",
    "   \n",
    "2. **Informaci√≥n err√≥nea (Misinformation)**\n",
    "   - Informaci√≥n falsa compartida **sin intenci√≥n** de enga√±ar.\n",
    "   - Ejemplo: Compartir un rumor que cre√≠ste verdadero.\n",
    "   \n",
    "3. **Malinformaci√≥n (Malinformation)**\n",
    "   - Informaci√≥n **verdadera** pero compartida para causar da√±o.\n",
    "   - Ejemplo: Filtraci√≥n de datos personales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32d41cf-cce3-48a5-a36e-f69273884393",
   "metadata": {},
   "source": [
    "## DeepFakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf87de9-3487-4b27-b65c-0b999f91143e",
   "metadata": {},
   "source": [
    "```{admonition} ¬øQu√© es una <strong>Deepfake</strong>?\n",
    "<div align=\"justify\">Es una palabra que es el resultado de: <strong>Deep Learning + Fake</strong>. Son videos, audios o im√°genes creados o alterados usando Inteligencia Artificial para hacer que una persona parezca decir o hacer algo que nunca hizo.</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16437035-2c0c-451c-8076-85edfbc478c4",
   "metadata": {},
   "source": [
    "### Tecnolog√≠a detr√°s de los Deepfakes:\n",
    "\n",
    "**Redes Neuronales Generativas Adversariales (GANs)**:\n",
    "- Una IA crea contenido falso\n",
    "- Otra IA intenta detectar si es falso\n",
    "- Compiten hasta que el falso es indistinguible del real\n",
    "\n",
    "### Ejemplo visual del proceso:\n",
    "\n",
    "**Pseudoc√≥digo** simplificado de c√≥mo funciona un deepfake:\n",
    "\n",
    "```python\n",
    "def crear_deepfake(video_original, rostro_objetivo):\n",
    "    # 1. Extraer rostro del video original\n",
    "    rostros = extraer_rostros(video_original)\n",
    "    \n",
    "    # 2. Mapear rostro objetivo sobre cada frame\n",
    "    for frame in rostros:\n",
    "        frame_falso = reemplazar_rostro(frame, rostro_objetivo)\n",
    "        ajustar_iluminacion(frame_falso)\n",
    "        ajustar_expresiones(frame_falso)\n",
    "    \n",
    "    # 3. Sincronizar audio con movimientos labiales\n",
    "    sincronizar_labios(frames_falsos, audio_original)\n",
    "    \n",
    "    return video_deepfake\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4159bf5-5821-4248-a929-aff924aafcf1",
   "metadata": {},
   "source": [
    "## Casos Hist√≥ricos Famosos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e699986-963b-4a12-91c7-72d5a260fd93",
   "metadata": {},
   "source": [
    "### 1. Deepfake de Obama (2018)\n",
    "\n",
    "<div align=\"justify\">El director Jordan Peele cre√≥ un deepfake del expresidente Barack Obama para demostrar los peligros de esta tecnolog√≠a {cite}`buzzfeed2018obama`. En el video, \"Obama\" dice cosas que nunca dijo realmente.</div>\n",
    "\n",
    "- **Objetivo**: Educar al p√∫blico sobre deepfakes.\n",
    "- **Resultado**: Millones de visualizaciones y conciencia sobre el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11bd95-c425-4354-a4a6-e590da5a03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('cQ54GDm1eL0', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab29a4c4-f0bf-4574-8af3-7bbbaf01b83a",
   "metadata": {},
   "source": [
    "### 2. Deepfake de Mark Zuckerberg (2019)\n",
    "\n",
    "**Contexto:** Instagram (propiedad de Facebook) permiti√≥ que un deepfake de su propio CEO permaneciera en la plataforma {cite}`susarla2019deepfake`.\n",
    "\n",
    "**Lo que dec√≠a el falso Zuckerberg:**\n",
    "> \"Imagina esto por un segundo: Un hombre, con control total de miles de millones de datos robados, todos sus secretos, sus vidas, sus futuros... Quien controla los datos, controla el futuro.\"\n",
    "\n",
    "**La iron√≠a:**\n",
    "- Facebook/Instagram **NO** elimin√≥ el video\n",
    "- Argumentaron que su pol√≠tica no requer√≠a borrar deepfakes\n",
    "- Esto gener√≥ enorme controversia\n",
    "\n",
    "**Temas que desat√≥:**\n",
    "1. üîê **Privacidad de datos** - ¬øCu√°nto sabe Facebook de nosotros?\n",
    "2. ‚öñÔ∏è **Responsabilidad de plataformas** - ¬øDeben eliminar deepfakes?\n",
    "3. üìú **Regulaci√≥n** - ¬øNecesitamos leyes espec√≠ficas?\n",
    "\n",
    "**Lecci√≥n clave:**\n",
    "Las grandes tecnol√≥gicas tambi√©n son vulnerables a su propia tecnolog√≠a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c9fc62-c5e4-4088-854b-cab06c9e0a9e",
   "metadata": {},
   "source": [
    "### 3. Guerra en Ucrania (2022)\n",
    "\n",
    "Uno de los primeros conflictos con **abundante desinformaci√≥n por IA** {cite}`disrupting2024deepfake`:\n",
    "\n",
    "- Deepfake de Zelensky \"rindi√©ndose\" (falso)\n",
    "- Videos viejos presentados como actuales\n",
    "- Im√°genes generadas por IA de supuestas atrocidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566b4af-d7a8-4ad4-8b80-b58af7ba239c",
   "metadata": {},
   "source": [
    "## Tipos de Deepfakes\n",
    "\n",
    "### 1. **Video**\n",
    "- Reemplazo de rostro completo.\n",
    "- Manipulaci√≥n de expresiones.\n",
    "- Ejemplo: Poner tu cara en una pel√≠cula.\n",
    "\n",
    "### 2. **Audio (Voice Cloning)**\n",
    "- Clonar voz de cualquier persona.\n",
    "- Solo necesitan ~3-10 segundos de audio original.\n",
    "- Aplicaciones: estafas telef√≥nicas, suplantaci√≥n.\n",
    "\n",
    "### 3. **Texto**\n",
    "- ChatGPT puede imitar estilos de escritura.\n",
    "- Generar art√≠culos \"de peri√≥dicos\" falsos.\n",
    "- Crear perfiles falsos en redes sociales.\n",
    "\n",
    "### 4. **Cheapfakes** (manipulaci√≥n simple)\n",
    "- No usan IA avanzada.\n",
    "- Edici√≥n b√°sica pero efectiva.\n",
    "- Ejemplo: Videos en c√°mara lenta para hacer que alguien parezca ebrio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00fc4a1-b58a-4ce2-b750-8c31877fc3a4",
   "metadata": {},
   "source": [
    "```{figure} Reconocimiento.jpeg\n",
    "---\n",
    "name: fig-reconocimiento-facial\n",
    "width: 80%\n",
    "alt: Tasas de error en reconocimiento facial\n",
    "---\n",
    "Tasas de error en sistemas de reconocimiento facial seg√∫n g√©nero y tono de piel: 0.8% para hombres de piel clara vs 34.7% para mujeres de piel oscura. Fuente: MIT Media Lab (2018).\n",
    "```\n",
    "\n",
    "Como se observa en la {numref}`fig-reconocimiento-facial`, los sistemas de reconocimiento facial muestran sesgos significativos seg√∫n g√©nero y tono de piel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125eb4a-c5c1-42db-8090-c9ca0224fb21",
   "metadata": {},
   "source": [
    "Seg√∫n {cite:t}`lisainstitute2024deepfakes`:\n",
    "\n",
    "> El impacto m√°s negativo que pueden causar los Deepfakes es generar en la sociedad una ausencia absoluta de confianza, lo que termine ocasionando un gran desinter√©s a la hora de diferenciar entre los archivos verdaderos y los Deepfakes. Cuando esta confianza es erosionada, se facilita mucho el hecho de generar dudas e inculcar ideas err√≥neas, dificultando el pensamiento cr√≠tico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f45e2-806b-4d34-abf1-ab9051d73ffe",
   "metadata": {},
   "source": [
    "## C√≥mo Detectar Deepfakes\n",
    "\n",
    "### Se√±ales visuales en videos: Checklist de detecci√≥n\n",
    "\n",
    "‚úì Parpadeo extra√±o o inexistente\n",
    "\n",
    "‚úì Movimientos labiales no sincronizados con el audio\n",
    "\n",
    "‚úì Cambios de iluminaci√≥n inconsistentes\n",
    "\n",
    "‚úì Bordes borrosos alrededor del rostro\n",
    "\n",
    "‚úì Artefactos digitales en el cabello\n",
    "\n",
    "‚úì Sombras que no corresponden\n",
    "\n",
    "‚úì Calidad de video inconsistente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referencias-deepfakes",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "```{bibliography}\n",
    ":filter: cited\n",
    ":style: author_year\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
