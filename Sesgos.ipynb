{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefc098f-dfc2-47fc-a3dd-d008c0bde5d5",
   "metadata": {},
   "source": [
    "# Sesgos Algorítmicos: El uso de Compas en el Sistemas de Justicia de EEUU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab62e7-df82-4d6c-8eac-08d8864bd617",
   "metadata": {},
   "source": [
    "```{admonition} Pregunta para reflexionar \n",
    "<div align=\"justify\"><strong>¿Puede una máquina ser racista? ¿Puede un algoritmo ser injusto?</strong></div><br> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128bf07b-0e7f-4d01-864f-d23af16614b0",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "<div align=\"justify\">Los algoritmos de Inteligencia Artificial aprenden de datos creados por humanos. Si esos datos contienen sesgos históricos, sociales o culturales, la IA los aprenderá y los replicará, a veces amplificándolos.Un ***sesgo algorítmico*** ocurre cuando un sistema de IA produce resultados sistemáticamente injustos o discriminatorios hacia ciertos grupos de personas.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97907251-86fc-4b55-b323-9dea9b974eab",
   "metadata": {},
   "source": [
    "## Tipos de Sesgos:\n",
    "\n",
    "1. **Sesgo en los datos de entrenamiento**\n",
    "   - Datos históricos que reflejan discriminación pasada\n",
    "   - Falta de representación de ciertos grupos\n",
    "\n",
    "2. **Sesgo en el diseño del algoritmo**\n",
    "   - Decisiones de programación que favorecen ciertos resultados\n",
    "   - Métricas de éxito mal definidas\n",
    "\n",
    "3. **Sesgo en la interpretación**\n",
    "   - Cómo se usan y se interpretan los resultados de la IA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a6f92-4f28-4006-8e50-ae38f6c47a68",
   "metadata": {},
   "source": [
    "## Caso Histórico: COMPAS y el Sistema de Justicia en EE.UU.\n",
    "\n",
    "### ¿Qué es COMPAS?\n",
    "\n",
    "<div align=\"justify\">\n",
    "<strong>COMPAS</strong> (Correctional Offender Management Profiling for Alternative Sanctions) es un algoritmo usado en varios estados de EE.UU. para predecir si un acusado volverá a cometer un crimen (reincidencia).\n",
    "</div><br>\n",
    "\n",
    "**Lo usa para**:\n",
    "\n",
    "- Decidir fianzas\n",
    "- Determinar sentencias\n",
    "- Evaluar libertad condicional\n",
    "\n",
    "### El Problema Descubierto\n",
    "\n",
    "En **2016**, una investigación de **ProPublica** reveló que COMPAS era **racialmente sesgado**:\n",
    "\n",
    "**Hallazgos claves**:<br>\n",
    "- Personas afroamericanas tenían **casi el doble** de probabilidad de ser incorrectamente clasificadas como \"alto riesgo\" de reincidencia\n",
    "- Personas blancas tenían más probabilidad de ser clasificadas como \"bajo riesgo\" incluso cuando sí reincidían\n",
    "- La precisión general era solo del **61%** (similar a lanzar una moneda)\n",
    "\n",
    "\n",
    "### Datos Concretos:\n",
    "\n",
    "| Grupo | Falsamente clasificados como \"Alto Riesgo\" | Falsamente clasificados como \"Bajo Riesgo\" |\n",
    "|:-------:|:---------------------------------------------:|:---------------------------------------------:|\n",
    "| Afroamericanos | **45%** | 23% |\n",
    "| Blancos | 23% | **48%** |\n",
    "\n",
    "### ¿Por qué ocurrió esto?\n",
    "\n",
    "<div align=\"justify\">El algoritmo fue entrenado con datos históricos del sistema de justicia de EE.UU., que tiene **sesgos documentados**:</div>\n",
    "\n",
    "1. **Arrestos desproporcionados** de minorías\n",
    "2. **Sentencias más duras** para personas afroamericanas\n",
    "3. **Mayor vigilancia** en vecindarios de bajos recursos\n",
    "\n",
    "**La IA aprendió estos patrones y los perpetuó.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70628cb-e525-43b6-90de-e0b2aa19f77a",
   "metadata": {},
   "source": [
    "## Otros Ejemplos de Sesgos Algorítmicos\n",
    "\n",
    "### 1. Reconocimiento Facial\n",
    "\n",
    "**Problema**: Sistemas de reconocimiento facial son menos precisos con personas de piel oscura y mujeres.\n",
    "\n",
    "**Estudio del MIT (2018)**:\n",
    "- Error del **0.8%** para hombres de piel clara\n",
    "- Error del **34.7%** para mujeres de piel oscura\n",
    "\n",
    "### 2. Sistemas de Contratación\n",
    "\n",
    "**Caso Amazon (2018)**:\n",
    "- Amazon desarrolló un sistema de IA para filtrar CVs\n",
    "- El sistema aprendió a **discriminar contra mujeres**\n",
    "- Penalizaba CVs con la palabra \"women\" (ej: \"women's chess club\")\n",
    "- Fue descontinuado\n",
    "\n",
    "### 3. Predicción de Rendimiento Académico\n",
    "\n",
    "**Problema**: Algoritmos que predicen deserción escolar pueden estar sesgados contra:\n",
    "- Estudiantes de bajos recursos\n",
    "- Estudiantes de zonas rurales\n",
    "- Minorías étnicas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c14f59-1f95-45ce-b551-a27cae0b0573",
   "metadata": {},
   "source": [
    "## ¿Cómo se crean estos Sesgos?\n",
    "\n",
    "### Ejemplo simplificado de cómo se crea un sesgo\n",
    "\n",
    "Datos históricos sesgados\n",
    "\n",
    "datos_entrenamiento = [\n",
    "    {\"nombre\": \"Juan\", \"barrio\": \"Las Condes\", \"arrestos\": 0},\n",
    "    {\"nombre\": \"María\", \"barrio\": \"Las Condes\", \"arrestos\": 0},\n",
    "    {\"nombre\": \"Pedro\", \"barrio\": \"La Pintana\", \"arrestos\": 2},\n",
    "    {\"nombre\": \"Ana\", \"barrio\": \"La Pintana\", \"arrestos\": 1},\n",
    "    # ... más datos sesgados\n",
    "]\n",
    "\n",
    "<div align=\"justify\">La IA aprende que \"barrio rico = menos arrestos\". Pero esto puede reflejar vigilancia desigual, no criminalidad real</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eadb24-e3ca-480e-8ce2-86f9fb5fa2b8",
   "metadata": {},
   "source": [
    "## Consecuencias Reales\n",
    "\n",
    "### En la Justicia:\n",
    "- ❌ Personas inocentes encarceladas por más tiempo\n",
    "- ❌ Fianzas más altas para ciertos grupos\n",
    "- ❌ Perpetuación de desigualdades históricas\n",
    "\n",
    "### En la Educación:\n",
    "- ❌ Estudiantes incorrectamente clasificados como \"bajo rendimiento\"\n",
    "- ❌ Menos oportunidades para ciertos grupos\n",
    "- ❌ Profecías autocumplidas\n",
    "\n",
    "### En el Empleo:\n",
    "- ❌ Discriminación en contratación\n",
    "- ❌ Falta de diversidad en empresas\n",
    "- ❌ Pérdida de talento\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c77f8a-d1f7-41b2-a861-217744f8218e",
   "metadata": {},
   "source": [
    "```{admonition} Reflexión Final: IA y Justicia Social \n",
    "<div align=\"justify\"><strong>\"Los algoritmos no son neutros. Son productos de decisiones humanas y datos históricos que reflejan nuestras sociedades imperfectas</strong>.\"\n",
    "- Cathy O'Neil, \"Weapons of Math Destruction\"</div> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c606a-eb2f-4db6-8fb5-09b1035b46a4",
   "metadata": {},
   "source": [
    "## ¿Cómo Combatir los Sesgos Algorítmicos?\n",
    "\n",
    "### 1. Datos Diversos y Representativos: \n",
    "\n",
    "Asegurarse de que los datos de entrenamiento incluyan:\n",
    "- Múltiples grupos demográficos\n",
    "- Contextos históricos diversos\n",
    "- Corrección de sesgos históricos\n",
    "\n",
    "### 2. Auditorías Independientes\n",
    "\n",
    "- Revisión externa de algoritmos\n",
    "- Transparencia en cómo funcionan\n",
    "- Publicación de tasas de error por grupo\n",
    "\n",
    "### 3. Equipos Diversos\n",
    "\n",
    "- Desarrolladores de diferentes orígenes\n",
    "- Perspectivas variadas en el diseño\n",
    "- Ética en el centro del desarrollo\n",
    "\n",
    "### 4. Regulación y Leyes\n",
    "\n",
    "- Leyes que exijan transparencia algorítmica\n",
    "- Derecho a explicación de decisiones automatizadas\n",
    "- Responsabilidad por daños causados por IA\n",
    "\n",
    "## Marco Legal Actual\n",
    "\n",
    "### Unión Europea: AI Act\n",
    "- Prohíbe ciertos usos de IA en justicia\n",
    "- Exige transparencia en sistemas de alto riesgo\n",
    "- Multas por incumplimiento\n",
    "\n",
    "### Chile:\n",
    "- Aún no existe legislación específica sobre sesgos en IA\n",
    "- Proyecto de ley de protección de datos personales en discusión\n",
    "- Necesidad de regulación urgente\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67293b6b-1439-4ff6-a117-b215afc6f1fb",
   "metadata": {},
   "source": [
    "## Actividad: Detecta el Sesgo: Ejercicio Práctico\n",
    "\n",
    "**Escenario**: Una universidad quiere usar IA para seleccionar estudiantes.\n",
    "\n",
    "**Datos disponibles**:\n",
    "- Notas de enseñanza media\n",
    "- Colegio de origen (público/privado)\n",
    "- Comuna de residencia\n",
    "- Actividades extracurriculares\n",
    "- Puntaje PSU\n",
    "\n",
    "**Preguntas**:\n",
    "1. ¿Qué sesgos podrían existir en estos datos?\n",
    "2. ¿Qué grupos podrían ser discriminados?\n",
    "3. ¿Cómo mejorarías el sistema?\n",
    "4. ¿Qué datos NO deberían usarse?\n",
    "\n",
    "<div align=\"justify\">\n",
    "La IA tiene el potencial de ser más justa que los humanos, pero solo si la diseñamos conscientemente para serlo. Debemos ser críticos y vigilantes sobre cómo se usa la tecnología en decisiones que afectan vidas humanas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56a2a3-337c-43b0-809e-f95c635959db",
   "metadata": {},
   "source": [
    "## Recursos para Profundizar\n",
    "\n",
    "- **Libro**: \"Weapons of Math Destruction\" - Cathy O'Neil\n",
    "- **Documental**: \"Coded Bias\" - Netflix\n",
    "- **Artículo**: ProPublica - \"Machine Bias\" (2016)\n",
    "- **Organización**: Algorithmic Justice League\n",
    "\n",
    "---\n",
    "\n",
    "**Próxima clase**: Exploraremos la desinformación y los deepfakes en la era digital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c5f112-420a-4a81-8dac-6931355e5946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
