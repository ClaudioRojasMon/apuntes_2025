{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefc098f-dfc2-47fc-a3dd-d008c0bde5d5",
   "metadata": {},
   "source": [
    "# Sesgos Algorítmicos: El uso de Compas en el Sistemas de Justicia de EEUU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab62e7-df82-4d6c-8eac-08d8864bd617",
   "metadata": {},
   "source": [
    "```{admonition} Pregunta para reflexionar\n",
    "<div align=\"justify\"><strong>¿Puede una máquina ser racista? ¿Puede un algoritmo ser injusto?</strong></div><br>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128bf07b-0e7f-4d01-864f-d23af16614b0",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "<div align=\"justify\">Los algoritmos de Inteligencia Artificial aprenden de datos creados por humanos. Si esos datos contienen sesgos históricos, sociales o culturales, la IA los aprenderá y los replicará, a veces amplificándolos {cite}`oneil2016weapons`. Un <strong>sesgo algorítmico</strong> ocurre cuando un sistema de IA produce resultados sistemáticamente injustos o discriminatorios hacia ciertos grupos de personas.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97907251-86fc-4b55-b323-9dea9b974eab",
   "metadata": {},
   "source": [
    "## Tipos de Sesgos:\n",
    "\n",
    "1. **Sesgo en los datos de entrenamiento**\n",
    "   - Datos históricos que reflejan discriminación pasada\n",
    "   - Falta de representación de ciertos grupos\n",
    "\n",
    "2. **Sesgo en el diseño del algoritmo**\n",
    "   - Decisiones de programación que favorecen ciertos resultados\n",
    "   - Métricas de éxito mal definidas\n",
    "\n",
    "3. **Sesgo en la interpretación**\n",
    "   - Cómo se usan y se interpretan los resultados de la IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322a6f92-4f28-4006-8e50-ae38f6c47a68",
   "metadata": {},
   "source": [
    "## Caso Histórico: COMPAS y el Sistema de Justicia en EE.UU.\n",
    "\n",
    "### ¿Qué es COMPAS?\n",
    "\n",
    "<div align=\"justify\">\n",
    "<strong>COMPAS</strong> (Correctional Offender Management Profiling for Alternative Sanctions) es un algoritmo usado en varios estados de EE.UU. para predecir si un acusado volverá a cometer un crimen (reincidencia).\n",
    "</div><br>\n",
    "\n",
    "**Lo usa para**:\n",
    "\n",
    "- Decidir fianzas\n",
    "- Determinar sentencias\n",
    "- Evaluar libertad condicional\n",
    "\n",
    "### El Problema Descubierto\n",
    "\n",
    "En **2016**, una investigación de **ProPublica** reveló que COMPAS era **racialmente sesgado** {cite}`angwin2016machine`:\n",
    "\n",
    "**Hallazgos claves**:\n",
    "\n",
    "- Personas afroamericanas tenían **casi el doble** de probabilidad de ser incorrectamente clasificadas como \"alto riesgo\" de reincidencia\n",
    "- Personas blancas tenían más probabilidad de ser clasificadas como \"bajo riesgo\" incluso cuando sí reincidían\n",
    "- La precisión general era solo del **61%** (similar a lanzar una moneda)\n",
    "\n",
    "```{figure} COMPAS.jpg\n",
    "---\n",
    "name: fig-compas-accuracy\n",
    "width: 70%\n",
    "alt: Precisión del algoritmo COMPAS\n",
    "---\n",
    "Precisión del algoritmo COMPAS en la predicción de reincidencia: aproximadamente 60-70% de aciertos, similar a lanzar una moneda. Fuente: {cite:t}`duke2017compas`.\n",
    "```\n",
    "\n",
    "Como se observa en la {numref}`fig-compas-accuracy`, la precisión del algoritmo COMPAS es sorprendentemente baja.\n",
    "\n",
    "### Datos Concretos:\n",
    "\n",
    "```{table} Tasas de error de COMPAS por grupo étnico\n",
    ":name: tabla-compas-errores\n",
    "\n",
    "| Grupo | Falsamente clasificados como \"Alto Riesgo\" | Falsamente clasificados como \"Bajo Riesgo\" |\n",
    "|:-------:|:---------------------------------------------:|:---------------------------------------------:|\n",
    "| Afroamericanos | **45%** | 23% |\n",
    "| Blancos | 23% | **48%** |\n",
    "```\n",
    "\n",
    "Los datos de la {numref}`tabla-compas-errores` muestran claramente el sesgo racial del algoritmo {cite}`angwin2016machine`.\n",
    "\n",
    "### ¿Por qué ocurrió esto?\n",
    "\n",
    "<div align=\"justify\">El algoritmo fue entrenado con datos históricos del sistema de justicia de EE.UU., que tiene <strong>sesgos documentados</strong> {cite}`oneil2016weapons`:</div>\n",
    "\n",
    "1. **Arrestos desproporcionados** de minorías\n",
    "2. **Sentencias más duras** para personas afroamericanas\n",
    "3. **Mayor vigilancia** en vecindarios de bajos recursos\n",
    "\n",
    "**La IA aprendió estos patrones y los perpetuó.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70628cb-e525-43b6-90de-e0b2aa19f77a",
   "metadata": {},
   "source": [
    "## Otros Ejemplos de Sesgos Algorítmicos\n",
    "\n",
    "### 1. Reconocimiento Facial\n",
    "\n",
    "**Problema**: Sistemas de reconocimiento facial son menos precisos con personas de piel oscura y mujeres {cite}`buolamwini2018gender`.\n",
    "\n",
    "**Estudio del MIT (2018)**:\n",
    "- Error del **0.8%** para hombres de piel clara\n",
    "- Error del **34.7%** para mujeres de piel oscura\n",
    "\n",
    "### 2. Sistemas de Contratación\n",
    "\n",
    "**Caso Amazon (2018)** {cite}`dastin2018amazon`:\n",
    "- Amazon desarrolló un sistema de IA para filtrar CVs\n",
    "- El sistema aprendió a **discriminar contra mujeres**\n",
    "- Penalizaba CVs con la palabra \"women\" (ej: \"women's chess club\")\n",
    "- Fue descontinuado\n",
    "\n",
    "### 3. Predicción de Rendimiento Académico\n",
    "\n",
    "**Problema**: Algoritmos que predicen deserción escolar pueden estar sesgados contra:\n",
    "- Estudiantes de bajos recursos\n",
    "- Estudiantes de zonas rurales\n",
    "- Minorías étnicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ec69d-053f-4be7-ae96-c99e973a3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Simulación simple de sesgo en datos\n",
    "import random\n",
    "\n",
    "def evaluar_candidato(nombre, universidad):\n",
    "    \"\"\"Algoritmo sesgado que favorece ciertas universidades\"\"\"\n",
    "    universidades_elite = ['Harvard', 'MIT', 'Stanford']\n",
    "    \n",
    "    puntaje_base = random.randint(50, 70)\n",
    "    \n",
    "    # SESGO: +30 puntos si viene de universidad \"elite\"\n",
    "    if universidad in universidades_elite:\n",
    "        puntaje_base += 30\n",
    "    \n",
    "    return puntaje_base\n",
    "\n",
    "# Ejemplos\n",
    "print(f\"Juan (U. Chile): {evaluar_candidato('Juan', 'U. Chile')}\")\n",
    "print(f\"María (Harvard): {evaluar_candidato('María', 'Harvard')}\")\n",
    "print(f\"Pedro (U. Concepción): {evaluar_candidato('Pedro', 'U. Concepción')}\")\n",
    "print(f\"Ana (MIT): {evaluar_candidato('Ana', 'MIT')}\")\n",
    "\n",
    "print(\"\\n¿Notas el sesgo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sesgo-ejemplo-analisis",
   "metadata": {},
   "source": [
    "### Análisis del Código:\n",
    "\n",
    "**El sesgo está en la línea**:\n",
    "```python\n",
    "if universidad in universidades_elite:\n",
    "    puntaje_base += 30\n",
    "```\n",
    "\n",
    "**Problemas**:\n",
    "1. Asume que estudiantes de ciertas universidades son mejores\n",
    "2. No considera mérito individual\n",
    "3. Perpetúa desigualdades de acceso a educación \"elite\"\n",
    "4. Discrimina contra talento de universidades regionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eda87e-94a3-4ea0-be15-daa11afd4925",
   "metadata": {},
   "source": [
    "## Cómo Combatir los Sesgos Algorítmicos\n",
    "\n",
    "### 1. Datos Diversos y Representativos:\n",
    "\n",
    "Asegurarse de que los datos de entrenamiento incluyan:\n",
    "- Múltiples grupos demográficos\n",
    "- Contextos históricos diversos\n",
    "- Corrección de sesgos históricos\n",
    "\n",
    "### 2. Auditorías Independientes\n",
    "\n",
    "- Revisión externa de algoritmos\n",
    "- Transparencia en cómo funcionan\n",
    "- Publicación de tasas de error por grupo\n",
    "\n",
    "### 3. Equipos Diversos\n",
    "\n",
    "- Desarrolladores de diferentes orígenes\n",
    "- Perspectivas variadas en el diseño\n",
    "- Ética en el centro del desarrollo\n",
    "\n",
    "### 4. Regulación y Leyes\n",
    "\n",
    "- Leyes que exijan transparencia algorítmica\n",
    "- Derecho a explicación de decisiones automatizadas\n",
    "- Responsabilidad por daños causados por IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980931d-5126-4286-af0f-493f57f0b4fb",
   "metadata": {},
   "source": [
    "## Marco Legal Actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a555d8c-bb04-4946-b3c7-92c893afcba1",
   "metadata": {},
   "source": [
    "### Unión Europea: AI Act\n",
    "\n",
    "La Unión Europea ha aprobado regulaciones estrictas sobre IA {cite}`euaiact2024`:\n",
    "\n",
    "- Prohíbe ciertos usos de IA en justicia\n",
    "- Exige transparencia en sistemas de alto riesgo\n",
    "- Multas por incumplimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce439c-ef79-4435-859b-eac619fd40d9",
   "metadata": {},
   "source": [
    "```{figure} ReglamentoEUR.jpeg\n",
    "---\n",
    "name: fig-eu-ai-act\n",
    "width: 80%\n",
    "alt: Reglamento de la Unión Europea sobre IA\n",
    "---\n",
    "Marco regulatorio de la Unión Europea para la Inteligencia Artificial (AI Act). Fuente: {cite:t}`euaiact2024`.\n",
    "```\n",
    "\n",
    "Como se muestra en la {numref}`fig-eu-ai-act`, la UE ha establecido un marco comprehensivo para regular la IA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f7490-e15d-407e-bb46-7b3d39fbcd5d",
   "metadata": {},
   "source": [
    "### Chile:\n",
    "\n",
    "- Aún no existe legislación específica sobre sesgos en IA\n",
    "- Nueva ley de protección de datos personales\n",
    "- Existe un proyecto de ley que pretende regular los sistemas de inteligencia artificial (IA) en Chile, buscando asegurar que el desarrollo y uso de estas tecnologías sea respetuoso de los derechos de las personas, fomentar la innovación y fortalecer la capacidad del Estado para actuar frente a sus riesgos y desafíos {cite}`chile2024ailaw,minciencia2024ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67293b6b-1439-4ff6-a117-b215afc6f1fb",
   "metadata": {},
   "source": [
    "## Actividad: Detecta el Sesgo - Ejercicio Práctico\n",
    "\n",
    "**Escenario**: Una universidad quiere usar IA para seleccionar estudiantes.\n",
    "\n",
    "**Datos disponibles**:\n",
    "- Notas de enseñanza media\n",
    "- Colegio de origen (público/Subvencionado/privado)\n",
    "- Comuna o ciudad de residencia\n",
    "- Actividades extracurriculares\n",
    "- Puntaje PAES\n",
    "- Foto del estudiante\n",
    "\n",
    "**Preguntas**:\n",
    "1. ¿Qué sesgos podrían existir en estos datos?\n",
    "2. ¿Qué grupos podrían ser discriminados?\n",
    "3. ¿Cómo mejorarías el sistema?\n",
    "4. ¿Qué datos NO deberían usarse?\n",
    "\n",
    "<div align=\"justify\">\n",
    "La IA tiene el potencial de ser más justa que los humanos, pero solo si la diseñamos conscientemente para serlo. Debemos ser críticos y vigilantes sobre cómo se usa la tecnología en decisiones que afectan vidas humanas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b56a2a3-337c-43b0-809e-f95c635959db",
   "metadata": {},
   "source": [
    "## Recursos para Profundizar\n",
    "\n",
    "- **Libro**: {cite:t}`oneil2016weapons` - \"Weapons of Math Destruction\"\n",
    "- **Documental**: \"Coded Bias\" - Netflix\n",
    "- **Artículo**: {cite:t}`angwin2016machine` - ProPublica \"Machine Bias\"\n",
    "- **Organización**: Algorithmic Justice League\n",
    "\n",
    "---\n",
    "\n",
    "**Próxima clase**: Exploraremos la desinformación y los deepfakes en la era digital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referencias-sesgos",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "```{bibliography}\n",
    ":filter: cited\n",
    ":style: author_year\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
